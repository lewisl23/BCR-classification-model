{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7bf318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.train import report\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae5ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training and testing sets\n",
    "X_train = pd.read_csv(\"/home/s2106664/msc_project/training_testing_dataset/X_train.csv\")\n",
    "X_validate = pd.read_csv(\"/home/s2106664/msc_project/training_testing_dataset/X_validate.csv\")\n",
    "X_test = pd.read_csv(\"/home/s2106664/msc_project/training_testing_dataset/X_test.csv\")\n",
    "y_train = pd.read_csv(\"/home/s2106664/msc_project/training_testing_dataset/y_train.csv\").squeeze()\n",
    "y_validate = pd.read_csv(\"/home/s2106664/msc_project/training_testing_dataset/y_validate.csv\").squeeze()\n",
    "y_test = pd.read_csv(\"/home/s2106664/msc_project/training_testing_dataset/y_test.csv\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de57a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7290722, 78)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff833b",
   "metadata": {},
   "source": [
    "# 1. Hyperparameter tuning using training set 5 folds cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87743689",
   "metadata": {},
   "source": [
    "## 1.1 Hyperparameter tuning without minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, layer_sizes):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in layer_sizes:\n",
    "            layers += [nn.Linear(prev, h), nn.LeakyReLU(negative_slope=0.01)]\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_mlp_cv(config, data=None):\n",
    "    X, y = data\n",
    "    X_np = np.array(X)\n",
    "    y = np.array(y)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Identify if cuda is available to use GPU\n",
    "    if torch.cuda.is_available() == True:\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    val_f1s = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_np, y):\n",
    "        X_train_df, X_val_df = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()\n",
    "        #X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # scalling the datasets\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        scaled_features = [\"hypermutation_rate\", \"cdr3_length\", \"Factor_I\", \"Factor_II\",\n",
    "                           \"Factor_III\", \"Factor_IV\", \"Factor_V\", \"np1_length\", \"np2_length\"]\n",
    "        X_train_scaled = X_train_df.copy()\n",
    "        X_train_scaled[scaled_features] = scaler.fit_transform(X_train_scaled[scaled_features])\n",
    "        X_val_scaled = X_val_df.copy()\n",
    "        X_val_scaled[scaled_features] = scaler.transform(X_val_scaled[scaled_features])\n",
    "\n",
    "        X_train_scaled = np.array(X_train_scaled)\n",
    "        X_val_scaled = np.array(X_val_scaled)\n",
    "        \n",
    "        # load datatset into GPU\n",
    "        X_train = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "        X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "        y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "        # Build model from config\n",
    "        layers = [config[\"layer_1_size\"]]\n",
    "        if config[\"n_layers\"] >= 2:\n",
    "            layers.append(config[\"layer_2_size\"])\n",
    "        if config[\"n_layers\"] == 3:\n",
    "            layers.append(config[\"layer_3_size\"])\n",
    "        if config[\"n_layers\"] == 4:\n",
    "            layers.append(config[\"layer_4_size\"])\n",
    "\n",
    "        model = MLP(input_dim=X.shape[1], output_dim=len(set(y)), layer_sizes=layers).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(250):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X_train)\n",
    "            loss = F.cross_entropy(out, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_out = model(X_val)\n",
    "                val_loss = F.cross_entropy(val_out, y_val).item()\n",
    "                preds = torch.argmax(val_out, dim=1).cpu().numpy()\n",
    "                true = y_val.cpu().numpy()\n",
    "                acc = accuracy_score(true, preds)\n",
    "                f1 = f1_score(true, preds, average=\"weighted\")\n",
    "            \n",
    "            # early stopper if patience reached\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_acc = acc\n",
    "                best_f1 = f1\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Patience reached\")\n",
    "                    break\n",
    "\n",
    "        val_losses.append(best_val_loss)\n",
    "        val_accuracies.append(best_acc)\n",
    "        val_f1s.append(best_f1)\n",
    "\n",
    "        # Clean the GPU memory between each model\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Report mean metrics to Ray Tune\n",
    "\n",
    "    metric = {\n",
    "        \"val_loss\" : np.mean(val_losses),\n",
    "        \"accuracy\" : np.mean(val_accuracies),\n",
    "        \"f1_score\" : np.mean(val_f1s)\n",
    "        }\n",
    "\n",
    "    tune.report(metrics=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982dd40c",
   "metadata": {},
   "source": [
    "## 1.2 Hyperparameter tuning using minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a271e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, layer_sizes):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in layer_sizes:\n",
    "            layers += [nn.Linear(prev, h), nn.LeakyReLU(negative_slope=0.01)]\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_mlp_cv(config, data=None):\n",
    "    X, y = data\n",
    "    X_np = np.array(X)\n",
    "    y = np.array(y)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Identify if cuda is available to use GPU\n",
    "    if torch.cuda.is_available() == True:\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    val_f1s = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_np, y):\n",
    "        X_train_df, X_val_df = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()\n",
    "        #X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # scalling the datasets\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        scaled_features = [\"hypermutation_rate\", \"cdr3_length\", \"Factor_I\", \"Factor_II\",\n",
    "                           \"Factor_III\", \"Factor_IV\", \"Factor_V\", \"np1_length\", \"np2_length\"]\n",
    "        X_train_scaled = X_train_df.copy()\n",
    "        X_train_scaled[scaled_features] = scaler.fit_transform(X_train_scaled[scaled_features])\n",
    "        X_val_scaled = X_val_df.copy()\n",
    "        X_val_scaled[scaled_features] = scaler.transform(X_val_scaled[scaled_features])\n",
    "\n",
    "        X_train_scaled = np.array(X_train_scaled)\n",
    "        X_val_scaled = np.array(X_val_scaled)\n",
    "        \n",
    "        # load datatset into GPU\n",
    "        X_train = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_val = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "        y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=config[\"batch_size\"],\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        # Build model from config\n",
    "        layers = [config[\"layer_1_size\"]]\n",
    "        if config[\"n_layers\"] >= 2:\n",
    "            layers.append(config[\"layer_2_size\"])\n",
    "        if config[\"n_layers\"] == 3:\n",
    "            layers.append(config[\"layer_3_size\"])\n",
    "        if config[\"n_layers\"] == 4:\n",
    "            layers.append(config[\"layer_4_size\"])\n",
    "\n",
    "        model = MLP(input_dim=X.shape[1], output_dim=len(set(y)), layer_sizes=layers).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(250):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = F.cross_entropy(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_out = model(X_val)\n",
    "                val_loss = F.cross_entropy(val_out, y_val).item()\n",
    "                preds = torch.argmax(val_out, dim=1).cpu().numpy()\n",
    "                true = y_val.cpu().numpy()\n",
    "                acc = accuracy_score(true, preds)\n",
    "                f1 = f1_score(true, preds, average=\"weighted\")\n",
    "            \n",
    "            print(f\"Epoch number {epoch} completed\")\n",
    "\n",
    "            # early stopper if patience reached\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_acc = acc\n",
    "                best_f1 = f1\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Patience reached\")\n",
    "                    break\n",
    "\n",
    "        val_losses.append(best_val_loss)\n",
    "        val_accuracies.append(best_acc)\n",
    "        val_f1s.append(best_f1)\n",
    "\n",
    "        # Clean the GPU memory between each model\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Report mean metrics to Ray Tune\n",
    "\n",
    "    metric = {\n",
    "        \"val_loss\" : np.mean(val_losses),\n",
    "        \"accuracy\" : np.mean(val_accuracies),\n",
    "        \"f1_score\" : np.mean(val_f1s)\n",
    "        }\n",
    "\n",
    "    tune.report(metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf30016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 12:05:00,607\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-07-06 12:05:01,446\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2025-07-06 12:05:01,448\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "[I 2025-07-06 12:05:01,453] A new study created in memory with name: optuna\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-07-06 18:54:50</td></tr>\n",
       "<tr><td>Running for: </td><td>06:49:49.42        </td></tr>\n",
       "<tr><td>Memory:      </td><td>53.1/125.6 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=2<br>Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: -0.5938747107982636<br>Logical resource usage: 4.0/32 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc                   </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  layer_1_size</th><th style=\"text-align: right;\">  layer_2_size</th><th style=\"text-align: right;\">  layer_3_size</th><th style=\"text-align: right;\">  layer_4_size</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  n_layers</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  f1_score</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_mlp_cv_b4b30e83</td><td>RUNNING   </td><td>129.215.170.52:3731094</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.00647875 </td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_mlp_cv_3c6c71d2</td><td>TERMINATED</td><td>129.215.170.52:3700136</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.000167059</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10789.8 </td><td style=\"text-align: right;\">  0.55761 </td><td style=\"text-align: right;\">  0.662099</td><td style=\"text-align: right;\">  0.659087</td></tr>\n",
       "<tr><td>train_mlp_cv_983d4b19</td><td>TERMINATED</td><td>129.215.170.52:3706728</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.00960672 </td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4015.19</td><td style=\"text-align: right;\">  0.719565</td><td style=\"text-align: right;\">  0.565405</td><td style=\"text-align: right;\">  0.50712 </td></tr>\n",
       "<tr><td>train_mlp_cv_bbafd359</td><td>TERMINATED</td><td>129.215.170.52:3708091</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">           512</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.00332646 </td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9208.99</td><td style=\"text-align: right;\">  0.63014 </td><td style=\"text-align: right;\">  0.604129</td><td style=\"text-align: right;\">  0.602909</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_mlp_cv pid=3700136)\u001b[0m Using device: cuda\n",
      "\u001b[36m(train_mlp_cv pid=3700136)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3700136)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3700136)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3700136)\u001b[0m Patience reached\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  f1_score</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_mlp_cv_3c6c71d2</td><td style=\"text-align: right;\">  0.662099</td><td style=\"text-align: right;\">  0.659087</td><td style=\"text-align: right;\">  0.55761 </td></tr>\n",
       "<tr><td>train_mlp_cv_983d4b19</td><td style=\"text-align: right;\">  0.565405</td><td style=\"text-align: right;\">  0.50712 </td><td style=\"text-align: right;\">  0.719565</td></tr>\n",
       "<tr><td>train_mlp_cv_bbafd359</td><td style=\"text-align: right;\">  0.604129</td><td style=\"text-align: right;\">  0.602909</td><td style=\"text-align: right;\">  0.63014 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_mlp_cv pid=3700136)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3706728)\u001b[0m Using device: cuda\n",
      "\u001b[36m(train_mlp_cv pid=3706728)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3706728)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3706728)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3706728)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3706728)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3708091)\u001b[0m Using device: cuda\n",
      "\u001b[36m(train_mlp_cv pid=3708091)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3708091)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3708091)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3708091)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3708091)\u001b[0m Patience reached\n",
      "\u001b[36m(train_mlp_cv pid=3731094)\u001b[0m Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 18:54:50,856\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-07-06 18:54:50,877\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/s2106664/msc_project/model_training/ray_tune_results/train_mlp_cv_2025-07-06_12-05-01' in 0.0200s.\n",
      "2025-07-06 18:55:00,885\tINFO tune.py:1041 -- Total run time: 24599.44 seconds (24589.40 seconds for the tuning loop).\n",
      "2025-07-06 18:55:00,886\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fb1f6d6c850>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"n_layers\": tune.choice([3, 4]),\n",
    "    \"layer_1_size\": tune.choice([512]),\n",
    "    \"layer_2_size\": tune.choice([256]),\n",
    "    \"layer_3_size\": tune.choice([128]),\n",
    "    \"layer_4_size\": tune.choice([64]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"batch_size\": tune.choice([32, 64, 128])\n",
    "}\n",
    "\n",
    "tune.run(\n",
    "    tune.with_parameters(train_mlp_cv, data=(X_train, y_train)),\n",
    "    config=search_space,\n",
    "    num_samples=5,\n",
    "    scheduler=ASHAScheduler(metric=\"val_loss\", mode=\"min\"),\n",
    "    search_alg=OptunaSearch(metric=\"val_loss\", mode=\"min\"),\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 1},\n",
    "    max_concurrent_trials=1,\n",
    "    storage_path=\"/home/s2106664/msc_project/model_training/ray_tune_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b801b62",
   "metadata": {},
   "source": [
    "Best parameter from ray tune:\n",
    "- batch size: 128\n",
    "- learning rate: 0.00012211471427518402\n",
    "- number of layer: 4\n",
    "- layer 1 size: 512\n",
    "- layer 2 size: 512\n",
    "- layer 3 size: 64\n",
    "- layer 4 size: 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0238f",
   "metadata": {},
   "source": [
    "# 2. Final model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ab49d",
   "metadata": {},
   "source": [
    "## 2.1 Model training using best hyperparameter from raytune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfed79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "def train_and_save_model(X_train_data, y_train_data, X_validate_data, y_validate_data,\n",
    "                         batch_size=64, learning_rate=0.00015, save_path=\"mlp_model.pt\"):\n",
    "\n",
    "    # Identify if cuda is available to use GPU\n",
    "    if torch.cuda.is_available() == True:\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = [\"hypermutation_rate\", \"cdr3_length\", \"Factor_I\", \"Factor_II\",\n",
    "                       \"Factor_III\", \"Factor_IV\", \"Factor_V\", \"np1_length\", \"np2_length\"]\n",
    "    \n",
    "    X_train_scaled = X_train_data.copy()\n",
    "    X_train_scaled[scaled_features] = scaler.fit_transform(X_train_scaled[scaled_features])\n",
    "    X_val_scaled = X_validate_data.copy()\n",
    "    X_val_scaled[scaled_features] = scaler.transform(X_val_scaled[scaled_features])\n",
    "\n",
    "    # Save scaler for later use in testing\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_data.values, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val_scaled.values, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_validate_data.values, dtype=torch.long).to(device)\n",
    "\n",
    "    # DataLoader\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = MLP(input_dim=X_train_tensor.shape[1], output_dim=len(set(y_train_data))).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    # save the best model\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, 251):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = F.cross_entropy(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(X_val_tensor)\n",
    "            val_loss = F.cross_entropy(val_out, y_val_tensor).item()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # Save the model checkpoint\n",
    "            checkpoint_dir = \"/home/s2106664/msc_project/model_training/MLP_checkpoints\"\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, f\"model_epoch_{epoch:03d}.pth\"))\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                print(f\"New best model found at epoch {epoch:03d} with val loss {val_loss:.4f}\")\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_model = model.state_dict()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save model state\n",
    "    save_path = \"/home/s2106664/msc_project/model_training/best_mlp_model.pth\"\n",
    "    torch.save(best_model, save_path)\n",
    "\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4797d1",
   "metadata": {},
   "source": [
    "## 2.2 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f9bd49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6682\n",
      "Test F1 Score: 0.6661\n"
     ]
    }
   ],
   "source": [
    "# Load scaler\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# Features to scale (same as training)\n",
    "scaled_features = [\"hypermutation_rate\", \"cdr3_length\", \"Factor_I\", \"Factor_II\",\n",
    "                   \"Factor_III\", \"Factor_IV\", \"Factor_V\", \"np1_length\", \"np2_length\"]\n",
    "\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[scaled_features] = scaler.transform(X_test_scaled[scaled_features])\n",
    "\n",
    "# Convert to tensor\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available() == True:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Initialize and load the saved model\n",
    "input_dim = X_test_tensor.shape[1]\n",
    "output_dim = len(set(y_test))\n",
    "model = MLP(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/s2106664/msc_project/model_training/best_mlp_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Move test data to device\n",
    "X_test_tensor = X_test_tensor.to(device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted = predicted.cpu().numpy()\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "f1 = f1_score(y_test, predicted, average='weighted')  # or 'macro', 'micro' based on your need\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31235683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(\"loss_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # ROC Curve\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = F.softmax(model(X_val_tensor), dim=1).cpu().numpy()\n",
    "        y_true = y_val_tensor.cpu().numpy()\n",
    "        y_onehot = label_binarize(y_true, classes=list(range(probs.shape[1])))\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(probs.shape[1]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_onehot[:, i], probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(probs.shape[1]):\n",
    "        plt.plot(fpr[i], tpr[i], label=f\"Class {i} (AUC = {roc_auc[i]:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Validation ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(\"roc_curve.png\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
